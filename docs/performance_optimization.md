# Performance Optimization Features

This document describes the advanced performance optimization features implemented in the GGUF Loader App, including automatic backend selection, GPU layer optimization, performance caching, and batch processing.

## Overview

The performance optimization system provides intelligent backend selection and configuration to maximize model loading and inference performance based on your hardware capabilities and usage patterns.

## Features

### 1. Automatic Backend Selection

The system automatically selects the optimal backend based on:

- **Model Size**: Larger models may benefit from different backends
- **Hardware Configuration**: Available GPUs, VRAM, CPU cores, and RAM
- **Performance History**: Past performance data for similar configurations
- **User Preferences**: Hardware preference settings (auto, GPU, CPU)

#### Backend Scoring Algorithm

Each available backend is scored based on:

- **Base Reliability Score**: Historical reliability of the backend
- **Hardware Compatibility**: GPU support and driver availability
- **Model Size Optimization**: Backend efficiency for the model size
- **Memory Requirements**: VRAM and RAM usage optimization
- **Performance History**: Success rates and speed metrics

#### Example Usage

```python
from app.core.performance_optimizer import PerformanceOptimizer
from app.core.hardware_detector import HardwareDetector

# Initialize optimizer
hardware_detector = HardwareDetector()
optimizer = PerformanceOptimizer(hardware_detector)
optimizer.start()

# Get optimal backend recommendation
recommendation = optimizer.get_optimal_backend(
    model_path="path/to/model.gguf",
    available_backends=['ctransformers', 'transformers', 'llamafile'],
    hardware_preference='auto'
)

print(f"Recommended backend: {recommendation.recommended_backend}")
print(f"Confidence: {recommendation.confidence_score:.2%}")
print(f"Expected performance: {recommendation.expected_performance}")
```

### 2. Dynamic GPU Layer Allocation

Optimizes the number of model layers loaded onto GPU memory based on:

- **Available VRAM**: Total and available GPU memory
- **Model Size**: Estimated memory requirements
- **Model Architecture**: Number of layers in the model
- **Performance History**: Optimal configurations from past usage

#### GPU Layer Optimization Algorithm

1. **Hardware Detection**: Identify available GPUs and VRAM
2. **Model Analysis**: Estimate model size and layer count
3. **Memory Calculation**: Calculate optimal layer distribution
4. **Caching**: Store optimal configurations for reuse

#### Example Usage

```python
# Optimize GPU layers for a specific model
optimal_layers = optimizer.optimize_gpu_layers(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    available_vram_mb=8192
)

print(f"Optimal GPU layers: {optimal_layers}")
# -1 = all layers, 0 = no GPU, positive number = specific layer count
```

### 3. Model-Specific Performance Caching

The system maintains a persistent cache of performance data including:

- **Model Profiles**: Size, architecture, quantization format
- **Backend Performance**: Loading times, memory usage, inference speed
- **Hardware Configurations**: GPU, CPU, and memory specifications
- **Success Rates**: Reliability metrics for different configurations

#### Cache Structure

```python
# Model Profile
{
    "model_hash": "abc123...",
    "size_mb": 7500,
    "parameter_count": 7000000000,
    "quantization": "Q4_0",
    "architecture": "llama"
}

# Backend Performance Profile
{
    "backend_name": "ctransformers",
    "load_time_ms": 2500.0,
    "memory_usage_mb": 6000,
    "tokens_per_second": 45.5,
    "success_rate": 1.0,
    "usage_count": 15
}
```

#### Example Usage

```python
# Record performance data
optimizer.record_performance(
    backend_name="ctransformers",
    model_path="path/to/model.gguf",
    load_time_ms=2500.0,
    memory_usage_mb=6000,
    tokens_per_second=45.5,
    success=True
)

# Get performance statistics
stats = optimizer.get_performance_stats(
    model_path="path/to/model.gguf"
)
```

### 4. Batch Processing Optimization

Optimizes multiple inference requests through intelligent batching:

- **Request Queuing**: Priority-based request ordering
- **Batch Formation**: Optimal batch sizes based on hardware
- **Parallel Processing**: Concurrent request handling
- **Resource Management**: Memory and GPU utilization optimization

#### Batch Processing Features

- **Priority Queuing**: Higher priority requests processed first
- **Adaptive Batching**: Batch size adjusts based on system load
- **Timeout Management**: Prevents requests from waiting too long
- **Error Handling**: Graceful handling of failed requests

#### Example Usage

```python
from app.core.performance_optimizer import BatchProcessor
from app.core.model_backends import GenerationConfig

# Initialize batch processor
batch_processor = BatchProcessor(max_batch_size=8, max_wait_time=0.1)
batch_processor.start()

# Submit requests
config = GenerationConfig(max_tokens=100, temperature=0.7)
request_id = batch_processor.submit_request(
    prompt="Your prompt here",
    generation_config=config,
    priority=1
)

# Get result
result = batch_processor.get_result(request_id, timeout=30.0)
```

## Configuration

### Performance Optimizer Settings

```python
# Initialize with custom settings
optimizer = PerformanceOptimizer(
    hardware_detector=hardware_detector,
    cache_file="custom_cache.json"
)

# Batch processor settings
batch_processor = BatchProcessor(
    max_batch_size=16,      # Maximum requests per batch
    max_wait_time=0.2       # Maximum wait time for batching
)
```

### Hardware Preferences

- **auto**: Automatically select best hardware (default)
- **gpu**: Prefer GPU acceleration when available
- **cpu**: Force CPU-only processing

### Backend Priority

Backends are prioritized by default reliability:

1. **ctransformers**: Easy installation, good GPU support
2. **transformers**: Excellent GPU support, active development
3. **llamafile**: Single executable, automatic optimization
4. **llama-cpp-python**: Direct GGUF support, installation challenges

## Performance Monitoring

### Metrics Collected

- **Loading Times**: Model loading duration
- **Memory Usage**: RAM and VRAM consumption
- **Inference Speed**: Tokens per second
- **Success Rates**: Operation reliability
- **Hardware Utilization**: CPU and GPU usage

### Performance Analysis

```python
# Get backend statistics
stats = optimizer.get_performance_stats()
print(f"Total models cached: {stats['total_models']}")
print(f"Backend profiles: {stats['total_backend_profiles']}")

# Get model-specific performance
model_stats = optimizer.get_performance_stats(
    model_path="path/to/model.gguf",
    backend_name="ctransformers"
)
```

## Best Practices

### 1. Hardware Optimization

- **GPU Memory**: Ensure sufficient VRAM for your models
- **System RAM**: Have adequate RAM for CPU fallback
- **Storage**: Use fast storage (SSD) for model files
- **Cooling**: Maintain proper GPU temperatures

### 2. Model Selection

- **Quantization**: Use appropriate quantization for your hardware
- **Model Size**: Balance model capability with hardware limits
- **Architecture**: Consider model architecture compatibility

### 3. Performance Tuning

- **Layer Allocation**: Let the system optimize GPU layers automatically
- **Batch Sizes**: Use default batch sizes unless you have specific needs
- **Context Length**: Adjust based on your use case and memory
- **Temperature**: Lower temperatures for more consistent performance

### 4. Monitoring and Maintenance

- **Performance Tracking**: Monitor performance metrics regularly
- **Cache Management**: Periodically review and clean performance cache
- **Hardware Updates**: Update drivers and system software
- **Backend Updates**: Keep backends updated for best performance

## Troubleshooting

### Common Issues

#### Poor Performance
- Check GPU utilization and memory usage
- Verify optimal backend selection
- Review GPU layer allocation
- Monitor system resource usage

#### Memory Issues
- Reduce GPU layers if VRAM is insufficient
- Lower context size or batch size
- Consider model quantization
- Check for memory leaks

#### Backend Selection Problems
- Verify backend installation and availability
- Check hardware compatibility
- Review performance history and cache
- Test different hardware preferences

### Performance Debugging

```python
# Enable detailed logging
import logging
logging.getLogger("performance").setLevel(logging.DEBUG)

# Get diagnostic information
recommendation = optimizer.get_optimal_backend(...)
print("Reasoning:", recommendation.reasoning)
print("Alternatives:", recommendation.alternative_backends)

# Check hardware detection
hardware_info = hardware_detector.get_hardware_info()
print("Hardware:", hardware_info.to_dict())
```

## API Reference

### PerformanceOptimizer

Main class for performance optimization coordination.

#### Methods

- `get_optimal_backend(model_path, available_backends, hardware_preference)`: Get backend recommendation
- `optimize_gpu_layers(model_path, backend_name, available_vram_mb)`: Optimize GPU layer allocation
- `record_performance(...)`: Record performance metrics
- `get_performance_stats(...)`: Get performance statistics

### ModelPerformanceCache

Manages persistent performance data storage.

#### Methods

- `store_model_profile(...)`: Store model metadata
- `store_backend_performance(...)`: Store performance data
- `get_model_profile(model_path)`: Retrieve model profile
- `get_backend_performance(model_path, backend_name)`: Retrieve performance data

### BatchProcessor

Handles batch processing of inference requests.

#### Methods

- `submit_request(prompt, generation_config, priority)`: Submit request
- `get_result(request_id, timeout)`: Get request result
- `start()`: Start batch processing
- `stop()`: Stop batch processing

## Integration Examples

### Basic Integration

```python
# Initialize components
hardware_detector = HardwareDetector()
optimizer = PerformanceOptimizer(hardware_detector)
optimizer.start()

# Load model with optimization
recommendation = optimizer.get_optimal_backend(
    model_path="model.gguf",
    available_backends=['ctransformers', 'transformers']
)

# Use recommended configuration
backend = create_backend(
    recommendation.recommended_backend,
    recommendation.recommended_config
)

# Record performance for future optimization
start_time = time.time()
result = backend.load_model("model.gguf")
load_time = (time.time() - start_time) * 1000

optimizer.record_performance(
    backend_name=recommendation.recommended_backend,
    model_path="model.gguf",
    load_time_ms=load_time,
    memory_usage_mb=get_memory_usage(),
    tokens_per_second=measure_inference_speed(),
    success=result.success
)
```

### Advanced Integration with UI

```python
class ModelManager:
    def __init__(self):
        self.optimizer = PerformanceOptimizer(HardwareDetector())
        self.optimizer.start()
    
    def load_model_optimized(self, model_path):
        # Get recommendation
        recommendation = self.optimizer.get_optimal_backend(
            model_path=model_path,
            available_backends=self.get_available_backends()
        )
        
        # Update UI with recommendation
        self.update_ui_recommendation(recommendation)
        
        # Load with optimal configuration
        return self.load_with_backend(
            recommendation.recommended_backend,
            recommendation.recommended_config
        )
    
    def update_ui_recommendation(self, recommendation):
        # Update UI elements
        self.backend_label.setText(recommendation.recommended_backend)
        self.confidence_bar.setValue(recommendation.confidence_score * 100)
        self.reasoning_text.setText('\n'.join(recommendation.reasoning))
```

This performance optimization system provides intelligent, adaptive performance tuning that learns from usage patterns and hardware capabilities to deliver optimal model loading and inference performance.

### 2. Dynamic GPU Layer Allocation

Automatically calculates the optimal number of GPU layers based on:

- **Available VRAM**: Total GPU memory available
- **Model Size**: Estimated memory requirements
- **Model Architecture**: Layer count estimation based on model type
- **Memory Overhead**: Backend-specific memory overhead calculations

#### GPU Layer Optimization Algorithm

```python
# Example: Optimize GPU layers for a model
optimal_layers = optimizer.optimize_gpu_layers(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    available_vram_mb=8192
)

print(f"Optimal GPU layers: {optimal_layers}")
# -1 = all layers on GPU
# 0 = CPU only
# >0 = specific number of layers on GPU
```

### 3. Model-Specific Performance Caching

The system maintains a persistent cache of performance data including:

- **Model Profiles**: Size, architecture, quantization information
- **Backend Performance**: Load times, memory usage, generation speed
- **Hardware Configurations**: GPU, CPU, and memory specifications
- **Success Rates**: Reliability metrics for different configurations

#### Performance Learning

```python
# Performance data is automatically recorded during usage
# Manual recording example:
optimizer.record_performance(
    backend_name="ctransformers",
    model_path="path/to/model.gguf",
    load_time_ms=2500.0,
    memory_usage_mb=6000,
    tokens_per_second=45.0,
    success=True
)

# Get performance statistics
stats = optimizer.get_performance_stats("path/to/model.gguf")
print(f"Cached models: {stats['total_models']}")
print(f"Backend profiles: {stats['total_backend_profiles']}")
```

### 4. Batch Processing Optimization

Intelligent batching of multiple requests with:

- **Priority Queuing**: High-priority requests processed first
- **Request Grouping**: Similar requests batched together
- **Adaptive Batching**: Batch size optimization based on system load
- **Timeout Management**: Configurable timeouts with fallback handling

#### Batch Processing Usage

```python
from app.core.model_backends import GenerationConfig

# Submit batch requests
config = GenerationConfig(max_tokens=100, temperature=0.7)

# High priority request
urgent_id = optimizer.batch_processor.submit_request(
    prompt="Urgent request",
    generation_config=config,
    priority=10
)

# Normal priority requests
request_ids = []
for prompt in ["Request 1", "Request 2", "Request 3"]:
    request_id = optimizer.batch_processor.submit_request(
        prompt=prompt,
        generation_config=config,
        priority=1
    )
    request_ids.append(request_id)

# Collect results
urgent_result = optimizer.batch_processor.get_result(urgent_id, timeout=30.0)
results = [optimizer.batch_processor.get_result(rid, timeout=30.0) for rid in request_ids]
```

### 5. Advanced Optimization Features

#### Dynamic Batch Size Calculation

```python
# Calculate optimal batch size based on system resources
optimal_batch_size = optimizer.get_dynamic_batch_size(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    available_memory_mb=16384,
    concurrent_requests=2
)
print(f"Optimal batch size: {optimal_batch_size}")
```

#### Adaptive Context Size Optimization

```python
# Optimize context size based on performance target
optimal_context = optimizer.get_adaptive_context_size(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    available_memory_mb=16384,
    target_performance="balanced"  # 'speed', 'balanced', 'quality'
)
print(f"Optimal context size: {optimal_context}")
```

#### Performance Prediction

```python
# Predict performance for untested configurations
hardware_config = {
    'gpu_count': 1,
    'total_vram': 8192,
    'cpu_cores': 8,
    'total_ram': 16384
}

prediction = optimizer.predict_performance(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    hardware_config=hardware_config
)

print("Performance prediction:")
print(f"  Load time: {prediction['predicted_load_time_ms']:.0f}ms")
print(f"  Memory usage: {prediction['predicted_memory_usage_mb']:.0f}MB")
print(f"  Tokens/second: {prediction['predicted_tokens_per_second']:.1f}")
print(f"  Success rate: {prediction['predicted_success_rate']:.2%}")
```

## Complete Integration Example

### Using the Performance-Integrated Backend Manager

```python
from app.core.performance_integration import PerformanceIntegratedBackendManager

# Initialize the performance-optimized backend manager
manager = PerformanceIntegratedBackendManager()

# Load model with comprehensive optimization
result = manager.load_model_optimized(
    model_path="path/to/your/model.gguf",
    hardware_preference="auto",      # 'auto', 'gpu', 'cpu'
    performance_target="balanced"    # 'speed', 'balanced', 'quality'
)

if result['success']:
    print(f"‚úÖ Model loaded successfully!")
    print(f"Backend: {result['backend_used']}")
    print(f"Load time: {result['load_time']:.1f}ms")
    print(f"Memory usage: {result['memory_usage']}MB")
    
    # Generate text with optimization
    response = manager.generate_text_with_optimization(
        prompt="Explain artificial intelligence in simple terms",
        max_tokens=200,
        temperature=0.7,
        use_batch_processing=True
    )
    print(f"Generated: {response}")
    
    # Generate multiple texts with batch optimization
    prompts = [
        "What is machine learning?",
        "Explain neural networks",
        "Define deep learning"
    ]
    
    batch_responses = await manager.generate_multiple_optimized(
        prompts=prompts,
        max_tokens=100,
        temperature=0.7
    )
    
    for i, response in enumerate(batch_responses):
        print(f"Response {i+1}: {response}")
```

## Performance Monitoring and Analytics

### Comprehensive Performance Insights

```python
# Get detailed performance statistics
insights = manager.get_comprehensive_performance_stats()

print("üìä Session Statistics:")
session_stats = insights['session_stats']
print(f"  Models loaded: {session_stats.get('models_loaded', 0)}")
print(f"  Total generations: {session_stats.get('total_generations', 0)}")
print(f"  Average generation time: {session_stats.get('avg_generation_time', 0):.2f}s")

print("üéØ Current Setup:")
current_model = insights['current_model']
if current_model:
    print(f"  Model: {Path(current_model['path']).name}")
    print(f"  Backend: {current_model['backend']}")

print("üí° Optimization Recommendations:")
recommendations = manager.get_optimization_recommendations()
for rec in recommendations.get('actionable_recommendations', []):
    print(f"  - {rec}")
```

### Performance Benchmarking

```python
# Run comprehensive benchmark
benchmark = manager.run_comprehensive_benchmark(num_iterations=5)

print("üèÉ Benchmark Results:")
avg_metrics = benchmark.get('average_metrics', {})
print(f"  Average tokens/second: {avg_metrics.get('overall_tokens_per_sec', 0):.1f}")
print(f"  Average response time: {avg_metrics.get('overall_response_time', 0):.2f}s")
print(f"  Success rate: {avg_metrics.get('success_rate', 0):.2%}")

# Analyze performance bottlenecks
analysis = benchmark.get('analysis', {})
if analysis.get('bottlenecks'):
    print("‚ö†Ô∏è Performance bottlenecks:")
    for bottleneck in analysis['bottlenecks']:
        print(f"    - {bottleneck}")

if analysis.get('optimization_suggestions'):
    print("üí° Optimization suggestions:")
    for suggestion in analysis['optimization_suggestions']:
        print(f"    - {suggestion}")
```

### Performance Reporting

```python
# Export comprehensive performance report
report_path = "performance_report.json"
manager.export_performance_report(report_path)
print(f"üìÑ Performance report exported to: {report_path}")

# Get optimization insights for analysis
model_insights = optimizer.get_optimization_insights("path/to/model.gguf")
print("üîç Optimization insights:")
print(f"  Recommendations: {len(model_insights['recommendations'])}")
print(f"  Performance trends: {len(model_insights['performance_trends'])}")
print(f"  Bottlenecks identified: {len(model_insights['bottlenecks'])}")
print(f"  Optimization opportunities: {len(model_insights['optimization_opportunities'])}")
```

## Configuration Options

### Performance Targets

- **`speed`**: Optimizes for fastest generation
  - Smaller context sizes (2048-4096)
  - Higher batch sizes for throughput
  - Reduced memory usage
  - Prioritizes backends with fast inference

- **`balanced`**: Balances speed and quality (default)
  - Moderate context sizes (4096-8192)
  - Balanced batch sizes
  - Good memory efficiency
  - Optimal for most use cases

- **`quality`**: Optimizes for best output quality
  - Larger context sizes (8192-16384)
  - Lower batch sizes for stability
  - Higher memory usage acceptable
  - Prioritizes backends with better quality

### Hardware Preferences

- **`auto`**: Automatically selects optimal hardware configuration
- **`gpu`**: Prefers GPU acceleration when available
- **`cpu`**: Forces CPU-only processing

### Advanced Configuration

```python
# Custom backend configuration
from app.core.model_backends import BackendConfig

custom_config = BackendConfig(
    name="ctransformers",
    gpu_enabled=True,
    gpu_layers=32,           # Specific GPU layer count
    context_size=4096,       # Context window size
    batch_size=512,          # Processing batch size
    threads=8,               # CPU thread count
    custom_args={
        "temperature": 0.7,
        "top_p": 0.9,
        "use_mmap": True
    }
)

# Apply custom configuration
manager.configs["ctransformers"] = custom_config
```

## Best Practices

### 1. System Learning and Optimization
- **Allow learning time**: Performance improves as the system builds performance history
- **Vary workloads**: Use different models and settings to build comprehensive data
- **Regular benchmarking**: Run periodic benchmarks to track performance trends
- **Monitor resource usage**: Keep track of GPU memory and system RAM utilization

### 2. Resource Management
- **Close unnecessary applications**: Free up GPU memory and system resources
- **Use appropriate model sizes**: Match model size to hardware capabilities
- **Monitor temperatures**: Ensure adequate cooling for sustained performance
- **Update drivers regularly**: Keep GPU drivers current for optimal performance

### 3. Configuration Management
- **Start with defaults**: Use 'auto' and 'balanced' settings initially
- **Adjust based on results**: Fine-tune based on benchmark results
- **Document working configurations**: Keep records of successful setups
- **Test configuration changes**: Benchmark before and after configuration changes

### 4. Performance Optimization Strategies
- **Use batch processing**: More efficient for multiple concurrent requests
- **Cache management**: Clear cache when hardware configuration changes significantly
- **Regular maintenance**: Periodically clear temporary files and restart services
- **Profile bottlenecks**: Use built-in diagnostics to identify performance issues

## Troubleshooting

### Common Issues and Solutions

#### 1. No Optimization Applied
**Symptoms**: 
- No optimization messages in application logs
- Random or suboptimal backend selection
- Performance not improving over time

**Solutions**:
```python
# Verify you're using the performance-integrated manager
from app.core.performance_integration import PerformanceIntegratedBackendManager
manager = PerformanceIntegratedBackendManager()

# Check optimization status
insights = manager.get_performance_insights()
cache_stats = insights.get('cache_statistics', {})
print(f"Optimization cache size: {cache_stats.get('optimization_cache_size', 0)}")
print(f"Models cached: {cache_stats.get('total_models_cached', 0)}")

# Verify performance optimizer is running
if hasattr(manager, 'performance_optimizer'):
    print("‚úÖ Performance optimizer is active")
else:
    print("‚ùå Performance optimizer not found")
```

#### 2. Poor Performance Recommendations
**Symptoms**:
- Low confidence scores in recommendations
- Suboptimal backend selection
- Performance worse than expected

**Solutions**:
```python
# Clear performance cache to reset learning
optimizer.clear_performance_cache()
print("Performance cache cleared - system will re-learn")

# Verify hardware detection is accurate
hardware_info = manager.hardware_detector.get_hardware_info()
print(f"Hardware detected:")
print(f"  GPUs: {hardware_info.gpu_count}")
print(f"  Total VRAM: {hardware_info.total_vram}MB")
print(f"  CPU cores: {hardware_info.cpu_cores}")
print(f"  Total RAM: {hardware_info.total_ram}MB")

# Manually record known good performance
optimizer.record_performance(
    backend_name="ctransformers",  # Replace with known good backend
    model_path="path/to/model.gguf",
    load_time_ms=1500.0,
    memory_usage_mb=4000,
    tokens_per_second=60.0,
    success=True
)
```

#### 3. GPU Not Utilized Effectively
**Symptoms**:
- GPU available but not used
- Low GPU utilization during inference
- Performance slower than expected with GPU

**Solutions**:
```python
# Check GPU layer optimization
hardware_info = manager.hardware_detector.get_hardware_info()
optimal_layers = optimizer.optimize_gpu_layers(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    available_vram_mb=hardware_info.total_vram
)
print(f"Optimal GPU layers: {optimal_layers}")

# Force GPU preference if auto-selection isn't working
result = manager.load_model_optimized(
    model_path="path/to/model.gguf",
    hardware_preference="gpu",  # Force GPU usage
    performance_target="speed"
)

# Check GPU memory usage
import subprocess
try:
    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'])
    print(f"GPU memory usage: {gpu_info.decode().strip()}")
except:
    print("Could not check GPU memory usage")
```

#### 4. Batch Processing Issues
**Symptoms**:
- Batch requests timing out
- Poor batch processing performance
- Requests not being processed

**Solutions**:
```python
# Check batch processor status
batch_stats = optimizer.batch_processor.get_queue_stats()
print("Batch processor status:")
print(f"  Running: {batch_stats['is_running']}")
print(f"  Pending requests: {batch_stats['pending_requests']}")
print(f"  Max batch size: {batch_stats['max_batch_size']}")
print(f"  Max wait time: {batch_stats['max_wait_time']}s")

# Restart batch processor if needed
if not batch_stats['is_running']:
    optimizer.batch_processor.stop()
    optimizer.batch_processor.start()
    print("Batch processor restarted")

# Adjust timeout for complex requests
result = optimizer.batch_processor.get_result(
    request_id, 
    timeout=60.0  # Increase timeout for complex requests
)
```

#### 5. Memory Issues
**Symptoms**:
- Out of memory errors
- System becoming unresponsive
- High memory usage warnings

**Solutions**:
```python
# Check memory usage and optimize
import psutil
memory = psutil.virtual_memory()
print(f"System memory: {memory.percent}% used")
print(f"Available: {memory.available // (1024**3)}GB")

# Optimize for memory-constrained systems
result = manager.load_model_optimized(
    model_path="path/to/model.gguf",
    hardware_preference="cpu",  # Use CPU to save VRAM
    performance_target="speed"  # Smaller context sizes
)

# Reduce batch size for memory efficiency
optimal_batch_size = optimizer.get_dynamic_batch_size(
    model_path="path/to/model.gguf",
    backend_name="ctransformers",
    available_memory_mb=memory.available // (1024**2),
    concurrent_requests=1  # Reduce concurrent requests
)
print(f"Memory-optimized batch size: {optimal_batch_size}")
```

### Diagnostic Commands

```python
def run_comprehensive_diagnostics():
    """Run complete performance optimization diagnostics."""
    print("üîç Performance Optimization Diagnostics")
    print("=" * 60)
    
    # Hardware detection
    detector = HardwareDetector()
    hardware_info = detector.get_hardware_info()
    print(f"üñ•Ô∏è  Hardware Configuration:")
    print(f"   GPUs detected: {hardware_info.gpu_count}")
    print(f"   Total VRAM: {hardware_info.total_vram}MB")
    print(f"   CPU cores: {hardware_info.cpu_cores}")
    print(f"   Total RAM: {hardware_info.total_ram}MB")
    print(f"   Recommended backend: {hardware_info.recommended_backend}")
    
    # Performance optimizer status
    optimizer = PerformanceOptimizer(detector)
    optimizer.start()
    
    print(f"\nüìä Performance Cache Status:")
    stats = optimizer.get_performance_stats()
    print(f"   Cached models: {stats['total_models']}")
    print(f"   Backend profiles: {stats['total_backend_profiles']}")
    print(f"   Cache file: {stats['cache_file']}")
    
    # Optimization insights
    print(f"\nüí° Optimization Analysis:")
    insights = optimizer.get_optimization_insights()
    print(f"   Optimization opportunities: {len(insights['optimization_opportunities'])}")
    print(f"   Performance bottlenecks: {len(insights['bottlenecks'])}")
    
    if insights['optimization_opportunities']:
        print("   Opportunities:")
        for opp in insights['optimization_opportunities']:
            print(f"     - {opp}")
    
    if insights['bottlenecks']:
        print("   Bottlenecks:")
        for bottleneck in insights['bottlenecks']:
            print(f"     - {bottleneck}")
    
    # Batch processor status
    print(f"\nüîÑ Batch Processing Status:")
    batch_stats = optimizer.batch_processor.get_queue_stats()
    print(f"   Processor running: {batch_stats['is_running']}")
    print(f"   Pending requests: {batch_stats['pending_requests']}")
    print(f"   Completed results: {batch_stats['completed_results']}")
    print(f"   Max batch size: {batch_stats['max_batch_size']}")
    
    # System resources
    print(f"\nüíæ System Resources:")
    import psutil
    memory = psutil.virtual_memory()
    print(f"   Memory usage: {memory.percent}%")
    print(f"   Available memory: {memory.available // (1024**3)}GB")
    
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        for i, gpu in enumerate(gpus):
            print(f"   GPU {i}: {gpu.memoryUtil*100:.1f}% memory used")
    except ImportError:
        print("   GPU monitoring not available (install gputil)")
    
    optimizer.stop()
    print(f"\n‚úÖ Diagnostics completed")

# Run the diagnostics
run_comprehensive_diagnostics()
```

## Performance Metrics and Monitoring

### Key Performance Indicators (KPIs)

The system tracks several important metrics:

1. **Loading Performance**:
   - Model load time (milliseconds)
   - Memory usage during loading (MB)
   - Success/failure rates

2. **Generation Performance**:
   - Tokens per second
   - First token latency
   - Response time for complete generation

3. **System Efficiency**:
   - GPU utilization percentage
   - Memory efficiency ratios
   - Backend switching frequency

4. **Optimization Effectiveness**:
   - Recommendation confidence scores
   - Performance improvement over time
   - Cache hit rates

### Monitoring Dashboard Data

```python
def get_performance_dashboard_data():
    """Get comprehensive performance data for monitoring dashboard."""
    manager = PerformanceIntegratedBackendManager()
    
    dashboard_data = {
        'system_status': {
            'hardware_info': manager.hardware_detector.get_hardware_info().to_dict(),
            'current_model': manager.current_model_path,
            'current_backend': manager.current_backend.config.name if manager.current_backend else None,
        },
        'performance_metrics': manager.get_comprehensive_performance_stats(),
        'optimization_insights': manager.get_optimization_recommendations(),
        'recent_activity': {
            'models_loaded_today': 0,  # Would be calculated from logs
            'generations_completed': 0,  # Would be calculated from session stats
            'average_performance': 0,   # Would be calculated from recent benchmarks
        }
    }
    
    return dashboard_data

# Example usage
dashboard = get_performance_dashboard_data()
print("Dashboard data ready for visualization")
```

This comprehensive performance optimization system ensures that your GGUF models run at peak efficiency regardless of your hardware configuration, continuously learning and adapting to provide the best possible performance for your specific use case.